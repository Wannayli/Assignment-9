{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wannayli/Assignment-9/blob/main/Building_the_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_GqRGAJfdqg",
        "outputId": "0fc60769-d1ef-43dc-970a-7c7be918ee55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk 11.0.18 2023-01-17\n",
            "OpenJDK Runtime Environment (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.18+10-post-Ubuntu-0ubuntu120.04.1, mixed mode, sharing)\n",
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!java --version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtMqb4Ihfgwu",
        "outputId": "4cc448b7-c48c-465e-81eb-3570d1b6d987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=9c97a094096b82cdf41e206d871bd8e10969408f04e69d7525a815704589501e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tBo2sHtg8gN9"
      },
      "outputs": [],
      "source": [
        "# Import Apache Spark SQL\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session/Context\n",
        "# We are using local machine with all the CPU cores [*]\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Hello Pyspark\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz1zUdJDftF3",
        "outputId": "5efc2fe8-f3b0-4a20-e387-77de49b5f908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fc64578b460>\n"
          ]
        }
      ],
      "source": [
        "# Check spark session\n",
        "print(spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VZhaiOkDhyCx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt-get update --fix-missing\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "#!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!mv spark-3.0.0-bin-hadoop3.2.tgz sparkkk\n",
        "!tar xf sparkkk\n",
        "!pip install -q findspark\n",
        "     \n",
        "\n",
        "#pip install spark\n",
        "     \n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('fpgrowth') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T36PsXKXdHJN"
      },
      "source": [
        "**Building the recommender**\n",
        "\n",
        "\n",
        "*   Loading and parsing the dataset. Persisting the resulting RDD for later use.\n",
        "\n",
        "*   Building the recommender model using the complete dataset. Persist the dataset for later use.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QjIp86ZdP2j"
      },
      "source": [
        "**File download**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EwoLZ8uqdoor"
      },
      "outputs": [],
      "source": [
        "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
        "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lc0H0VibiF3f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "datasets_path = os.path.join('/content', 'datasets')\n",
        "\n",
        "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
        "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J0ZVxLVxiaVo"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "small_f = urllib.request.urlretrieve(small_dataset_url, small_dataset_path)\n",
        "complete_f = urllib.request.urlretrieve(complete_dataset_url, complete_dataset_path)\n",
        "     \n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gj4JU8INksDS"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(small_dataset_path, \"r\") as z:\n",
        "    z.extractall(datasets_path)\n",
        "\n",
        "with zipfile.ZipFile(complete_dataset_path, \"r\") as z:\n",
        "    z.extractall(datasets_path)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7iIctlroTAV"
      },
      "source": [
        "Loading and parsing datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AlPJImG4oPCR"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate();\n",
        "\n",
        "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
        "\n",
        "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
        "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xo7jFtxbocpo"
      },
      "outputs": [],
      "source": [
        "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aNh5BuhYpEXF"
      },
      "outputs": [],
      "source": [
        "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7QWt_rpQn4",
        "outputId": "c64a06f9-de72-4e58-a40d-92ac01b0df77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '1', '4.0'), ('1', '3', '4.0'), ('1', '6', '4.0')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "small_ratings_data.take(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhRjhgmBpbFc",
        "outputId": "5f0a5607-ca48-4a0d-f0b1-299b1197586b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', 'Toy Story (1995)'),\n",
              " ('2', 'Jumanji (1995)'),\n",
              " ('3', 'Grumpier Old Men (1995)')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
        "\n",
        "small_movies_raw_data = sc.textFile(small_movies_file)\n",
        "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
        "\n",
        "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
        "    \n",
        "small_movies_data.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n15erC4epgnb"
      },
      "source": [
        "Collaborative Filtering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5DeZhC8pm2o"
      },
      "source": [
        "Selecting ALS parameters using the small dataset\n",
        "\n",
        "\n",
        "In order to determine the best ALS parameters, we will use the small dataset. We need first to split it into train, validation, and test datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4BgdHxcCp2X6"
      },
      "outputs": [],
      "source": [
        "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)\n",
        "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
        "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY1wYsWBqIFF"
      },
      "source": [
        "Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq5igtgsqJkR",
        "outputId": "52fef5dd-a47c-4541-f317-a4b75d021e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For rank 4 with iteration value 5 the RMSE is 0.9216828115540059\n",
            "For rank 4 with iteration value 10 the RMSE is 0.9121002114021121\n",
            "For rank 4 with iteration value 20 the RMSE is 0.9084344829867742\n",
            "For rank 8 with iteration value 5 the RMSE is 0.9252257022633836\n",
            "For rank 8 with iteration value 10 the RMSE is 0.9184327213070025\n",
            "For rank 8 with iteration value 20 the RMSE is 0.9154033053129291\n",
            "For rank 12 with iteration value 5 the RMSE is 0.9251850957336817\n",
            "For rank 12 with iteration value 10 the RMSE is 0.9160151537868968\n",
            "For rank 12 with iteration value 20 the RMSE is 0.9109898805756902\n",
            "The best model was trained with rank 4 with iteration value 20\n"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.recommendation import ALS\n",
        "import math\n",
        "\n",
        "seed = 5\n",
        "\n",
        "# we will use the 3 value as the same as the value suggest in assignment 9 which is 5 10 20\n",
        "\n",
        "iterations_d = [5, 10 ,20] # iteration value for addtional inner loop \n",
        "\n",
        "\n",
        "regularization_parameter = 0.1\n",
        "ranks = [4, 8, 12]\n",
        "errors = [0, 0, 0, 0, 0, 0, 0 ,0 ,0]\n",
        "err = 0\n",
        "tolerance = 0.02\n",
        "\n",
        "\n",
        "\n",
        "min_error = float('inf')\n",
        "best_rank = -1\n",
        "best_iteration = -1\n",
        "best_iteration_value = -1\n",
        "for rank in ranks:\n",
        "    for it in iterations_d:\n",
        "      model = ALS.train(training_RDD, rank, seed=seed, iterations=it,\n",
        "                        lambda_=regularization_parameter)\n",
        "      predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "      rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "      error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "      errors[err] = error\n",
        "      err += 1\n",
        "      print('For rank ' + str(rank) + ' with iteration value ' + str(it) +' the RMSE is ' + str(error) )\n",
        "      if error < min_error:\n",
        "          min_error = error\n",
        "          best_rank = rank\n",
        "          best_iteration_value = it\n",
        "\n",
        "print('The best model was trained with rank ' + str(best_rank) + ' with iteration value ' + str(best_iteration_value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWlTLWeSqh-b"
      },
      "source": [
        "Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOAR0E8FqqnW",
        "outputId": "38665b0f-f9ec-48d6-8d27-08b780cf70bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((372, 1084), 3.6273558036170312),\n",
              " ((4, 1084), 3.806848948516306),\n",
              " ((402, 1084), 3.4207522972274793)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "predictions.take(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlAkR0Hqq20r",
        "outputId": "485a33d6-fe2d-46da-ace5-3de629dc3946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((1, 457), (5.0, 4.505973514879727)),\n",
              " ((1, 1025), (5.0, 4.598382643002677)),\n",
              " ((1, 1089), (5.0, 4.855986245944868))]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "rates_and_preds.take(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXQ5B-cqq38M",
        "outputId": "de01516d-1e30-4e00-b84e-0041fd81accc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For testing data the RMSE is 0.9098996087060645\n"
          ]
        }
      ],
      "source": [
        "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=best_iteration_value,\n",
        "                      lambda_=regularization_parameter)\n",
        "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "    \n",
        "print('For testing data the RMSE is ' + str(error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0qFuf4rDPK"
      },
      "source": [
        "Using the complete dataset to build the final model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG6UYTt8rFCx",
        "outputId": "051e2e85-fac1-43ce-d1c0-b3ed0cec0d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 27753444 recommendations in the complete dataset\n"
          ]
        }
      ],
      "source": [
        "# Load the complete dataset file\n",
        "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
        "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
        "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
        "\n",
        "# Parse\n",
        "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
        "    \n",
        "print(\"There are \" + str(complete_ratings_data.count()) + \" recommendations in the complete dataset\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3HnG9Nwiq_ig"
      },
      "outputs": [],
      "source": [
        "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)\n",
        "\n",
        "complete_model = ALS.train(training_RDD, best_rank, seed=seed, \n",
        "                           iterations=best_iteration_value, lambda_=regularization_parameter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg6GkhDZuotH",
        "outputId": "6469defe-91b9-4729-9217-25b2149cf5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For testing data the RMSE is 0.8320316099023749\n"
          ]
        }
      ],
      "source": [
        "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "    \n",
        "print ('For testing data the RMSE is %s' % (error))\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9hAx7ZVynap"
      },
      "source": [
        "Make recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uLp3yaHyoT2",
        "outputId": "4b322790-8ed7-4599-c856-1976e505824c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 58098 movies in the complete dataset\n"
          ]
        }
      ],
      "source": [
        "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
        "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
        "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
        "\n",
        "# Parse\n",
        "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
        "\n",
        "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
        "    \n",
        "print (\"There are %s movies in the complete dataset\" % (complete_movies_titles.count()))\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Lo3mw5xKzgOJ"
      },
      "outputs": [],
      "source": [
        "def get_counts_and_averages(ID_and_ratings_tuple):\n",
        "    nratings = len(ID_and_ratings_tuple[1])\n",
        "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
        "\n",
        "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
        "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
        "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Emjvhvzi79"
      },
      "source": [
        "Adding new user ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58bfJn1-zlPW",
        "outputId": "ba996a4f-c540-49f6-fb9c-9e41be7bcbc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]\n"
          ]
        }
      ],
      "source": [
        "new_user_ID = 0\n",
        "\n",
        "# The format of each line is (userID, movieID, rating)\n",
        "new_user_ratings = [\n",
        "     (0,260,9), # Star Wars (1977)\n",
        "     (0,1,8), # Toy Story (1995)\n",
        "     (0,16,7), # Casino (1995)\n",
        "     (0,25,8), # Leaving Las Vegas (1995)\n",
        "     (0,32,9), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
        "     (0,335,4), # Flintstones, The (1994)\n",
        "     (0,379,3), # Timecop (1994)\n",
        "     (0,296,7), # Pulp Fiction (1994)\n",
        "     (0,858,10) , # Godfather, The (1972)\n",
        "     (0,50,8) # Usual Suspects, The (1995)\n",
        "    ]\n",
        "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
        "print ('New user ratings: %s' % new_user_ratings_RDD.take(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0LXBciidzufA"
      },
      "outputs": [],
      "source": [
        "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myTRlIlEzzAY",
        "outputId": "9fad304d-f09a-4ffc-c974-fb990c6f7d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New model trained in 507.276 seconds\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "t0 = time()\n",
        "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, \n",
        "                              iterations=best_iteration_value, lambda_=regularization_parameter)\n",
        "tt = time() - t0\n",
        "\n",
        "print(\"New model trained in \" + str(round(tt,3)) + \" seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laGLENV9zzbe"
      },
      "source": [
        "Getting top recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Hs4LIEy7z1yi"
      },
      "outputs": [],
      "source": [
        "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
        "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
        "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
        "\n",
        "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
        "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6XyZPlDy6ySs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5975ca9b-19ae-4bd1-d47e-8ebb90a3961b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(6216,\n",
              "  ((7.026445662645672, 'Nowhere in Africa (Nirgendwo in Afrika) (2001)'),\n",
              "   717)),\n",
              " (124320, ((7.492523897937501, 'Once a Thief (1965)'), 1)),\n",
              " (83916, ((6.498638531131836, 'Blues in the Night (1941)'), 9))]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
        "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
        "new_user_recommendations_rating_title_and_count_RDD = \\\n",
        "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
        "new_user_recommendations_rating_title_and_count_RDD.take(3)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uzGF9DvX7Dec"
      },
      "outputs": [],
      "source": [
        "new_user_recommendations_rating_title_and_count_RDD = \\\n",
        "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "yd-0B6Uo7HFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15631319-2dac-45e3-ded4-7cfc4f5dc435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP recommended movies (with more than 25 reviews):\n",
            "('Music for One Apartment and Six Drummers (2001)', 9.127790682513762, 31)\n",
            "('Rabbit of Seville (1950)', 8.981667585474511, 30)\n",
            "('\"Human Condition III', 8.924352265935553, 91)\n",
            "('Baseball (1994)', 8.825348145082458, 42)\n",
            "('Harakiri (Seppuku) (1962)', 8.82350612380764, 679)\n",
            "('Connections (1978)', 8.80328925260546, 49)\n",
            "('\"I', 8.761593869238741, 85)\n",
            "(\"Jim Henson's The Storyteller (1989)\", 8.737766561534556, 36)\n",
            "('Wow! A Talking Fish! (1983)', 8.733375962558078, 47)\n",
            "('\"Last Lions', 8.731667090276083, 38)\n",
            "('Duck Amuck (1953)', 8.720967892241077, 226)\n",
            "('Elway To Marino (2013)', 8.720128372049018, 25)\n",
            "('\"Lonely Wife', 8.71044688710834, 43)\n",
            "('Cosmos', 8.701890553829813, 157)\n",
            "('The Garden of Sinners - Chapter 5: Paradox Paradigm (2008)', 8.689368679793624, 27)\n",
            "('Rabbit Fire (1951)', 8.678015012489311, 46)\n",
            "('Dimensions of Dialogue (Moznosti dialogu) (1982)', 8.67314498984205, 65)\n",
            "('Crooks in Clover (a.k.a. Monsieur Gangster) (Les tontons flingueurs) (1963)', 8.666302841107914, 52)\n",
            "('\"In the blue sea', 8.661191712900077, 37)\n",
            "('\"Great War', 8.659008343795655, 31)\n",
            "('Seven Samurai (Shichinin no samurai) (1954)', 8.606794929069231, 14578)\n",
            "('\"Godfather', 8.604666177681054, 60904)\n",
            "('Ikiru (1952)', 8.601377373131328, 1551)\n",
            "('\"Century of the Self', 8.589141440685239, 213)\n",
            "('The War (2007)', 8.57146593019801, 53)\n"
          ]
        }
      ],
      "source": [
        "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
        "\n",
        "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
        "        '\\n'.join(map(str, top_movies)))\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAP6e3GJ7HmJ"
      },
      "source": [
        "Getting individual ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "jRyUvuOb7JrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c2f5521-3158-4dc0-a68d-8d951cc935b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Rating(user=0, product=116688, rating=2.0178150675852966)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
        "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
        "individual_movie_rating_RDD.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
        "\n",
        "model_path = os.path.join('..', 'models', 'movie_lens_als')\n",
        "\n",
        "# Save and load model\n",
        "model.save(sc, model_path)\n",
        "same_model = MatrixFactorizationModel.load(sc, model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "atpFR45ZEGKi",
        "outputId": "3fd66161-dd52-4137-dd32-270ed28f781c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38283)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JNetworkError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-977c50427643>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save and load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msame_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatrixFactorizationModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/pyspark/mllib/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sc, path)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a basestring, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:38283)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdvYwhOodG5INMn7ABwMUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}